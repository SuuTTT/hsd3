distributed:
  num_actors: 1
  num_learners: 1
  rdvu_path: /tmp
model:
  pi: pi_d2rl_d_1024
  q: qd_d2rl_d_1024
  reachability: q_d2rl_d_1024
optim:
  pi:
    _target_: torch.optim.Adam
    lr: 0.0001
    fuse: true
  q:
    _target_: torch.optim.Adam
    lr: 0.0001
    fuse: true
  reachability:
    _target_: torch.optim.Adam
    lr: 0.0001
    fuse: true

[2023-05-13 13:06:48,304][__main__][INFO] - Creating process group of size 2 via file:///tmp/rdvu-820457ed-71ba-4d37-856d-54106f83c4fe [rank=0]
[2023-05-13 13:06:48,306][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
Process Process-2:
Traceback (most recent call last):
  File "/home/sudingli/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/sudingli/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sudingli/workplace/hsd3/pretrain.py", line 875, in worker
    th.cuda.set_device(rank)
  File "/home/sudingli/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py", line 350, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[2023-05-13 13:06:58,316][torch.distributed.distributed_c10d][INFO] - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)
... hundreds of repeated [INFO] log ...
[2023-05-13 13:36:39,365][torch.distributed.distributed_c10d][INFO] - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)
Process Process-1:
Traceback (most recent call last):
  File "/home/sudingli/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/sudingli/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sudingli/workplace/hsd3/pretrain.py", line 879, in worker
    dist.init_process_group(
  File "/home/sudingli/miniconda3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 932, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/home/sudingli/miniconda3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 469, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)
(base) sudingli@c221:~/workplace/hsd3$ ^C

